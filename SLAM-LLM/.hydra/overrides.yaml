- ++train_config.enable_fsdp=false
- ++train_config.enable_ddp=true
- ++fsdp_config.pure_bf16=true
- ++model_config.llm_name=Qwen2.5-7B
- ++model_config.llm_path=/data_a100/models/GemmaX2-28-9B-v0.1
- ++model_config.llm_dim=4096
- ++model_config.query_len=80
- ++model_config.encoder_name=whisper
- ++model_config.encoder_projector_ds_rate=5
- ++model_config.encoder_path=
- ++model_config.encoder_path_hf=/data_a100/models/whisper-large-v3
- ++model_config.encoder_dim=1280
- ++model_config.encoder_projector=q-former
- ++model_config.beam=5
- ++dataset_config.dataset=st_dataset
- ++dataset_config.file=examples/st_covost2/dataset/fleurs_dataset.py:get_speech_dataset
- ++dataset_config.val_data_path=
- ++dataset_config.input_type=mel
- ++dataset_config.fix_length_audio=80
- ++dataset_config.mel_size=128
- ++dataset_config.inference_mode=true
- ++dataset_config.source=wmt24_all
- ++dataset_config.mode=mmt
- ++dataset_config.validnum=-2
- ++train_config.model_name=asr
- ++train_config.freeze_encoder=true
- ++train_config.freeze_llm=false
- ++train_config.batching_strategy=custom
- ++train_config.num_epochs=1
- ++train_config.val_batch_size=4
- ++train_config.num_workers_dataloader=8
- ++log_config.decode_log=/mgData3/zhaozhiyuan/vits/hit/code/SLAM-LLM/models/output/asr-7B-mi-28lang-mmt-lora-312/asr_epoch_1_step_5000/wmt24_all.jsonl
- ++ckpt_path=/mgData3/zhaozhiyuan/vits/hit/code/SLAM-LLM/models/output/asr-7B-mi-28lang-mmt-lora-312/asr_epoch_1_step_5000/model.pt
- ++train_config.use_peft=true
